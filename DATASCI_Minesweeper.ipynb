{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bB4bXbpKYSyA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "# Personalized class for O(1) remove and O(1) random access\n",
        "class ListDict:\n",
        "    def __init__(self, list):\n",
        "        self.list = list\n",
        "        self.dict = {i:i for i in list}\n",
        "    def remove(self, val):\n",
        "        index = self.dict[val]\n",
        "        lastElement = self.list.pop()\n",
        "        if index != len(self.list):\n",
        "            self.list[index] = lastElement\n",
        "            self.dict[lastElement] = index\n",
        "    def getRandom(self):\n",
        "        return self.list[random.randint(0, len(self.list) - 1)]\n",
        "\n",
        "# Self-coded Minesweeper allows for thousands of games to be played in tandem\n",
        "class MineSweeper:\n",
        "    def __init__(self, height, width, mines, batchSize):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.mines = mines\n",
        "        self.batchSize = batchSize\n",
        "\n",
        "        # Board is what is inputted into the model and solutionBoard is the y_true. Board is one-hot encoded\n",
        "        # to distinguish between cells in different states\n",
        "        self.board = tf.keras.utils.to_categorical(np.full((batchSize, height, width), -1.0), num_classes=10)\n",
        "\n",
        "        # Most board information is kept flattened as to make calculations easier, 2D representation is only\n",
        "        # required for convolution layers and visual output. Different numbers represent different values\n",
        "        # in each board as well in order to simplify calculations\n",
        "        self.solutionBoard = np.ones((batchSize, height*width))\n",
        "        self.allowedChoices = np.zeros((batchSize, height*width))\n",
        "        self.remainingMoves = [ListDict([i for i in range(height*width)]) for _ in range(batchSize)]\n",
        "\n",
        "        # Optimization strategy for counting the number of mines surrounding a grid is\n",
        "        # to precompute what the finished board would look like\n",
        "        # Start out with a padded matrix and use a 3x3 kernel to increment the area surrounding each mine by\n",
        "        # 1 for each mine and then delete the padding afterwards\n",
        "        self.solvedBoard = np.zeros((batchSize, height+2, width+2))\n",
        "        self.kernel = [0, 1, 2, width+2, width+3, width+4, 2*width+4, 2*width+5, 2*width+6]\n",
        "\n",
        "    def initializeBoard(self, prediction):\n",
        "        mines = []\n",
        "        y = prediction // self.width\n",
        "        x = prediction % self.width\n",
        "        self.board[:, y, x, 9] = 0\n",
        "        self.board[:, y, x, 0] = 1\n",
        "        self.allowedChoices[:, prediction] = 2\n",
        "\n",
        "        # The first grid that is selected must always be free and not be surrounded by any mines.\n",
        "        exclude = [i for i in range(max(y*self.width+x-1, y*self.width), min(y*self.width+x+2,(y+1)*self.width))]\n",
        "        lowerExclude = [i - self.width for i in exclude]\n",
        "        upperExclude = [i + self.width for i in exclude]\n",
        "        if y > 0: exclude = lowerExclude + exclude\n",
        "        if y < self.height-1: exclude = exclude + upperExclude\n",
        "        choices = np.setdiff1d(range(self.height*self.width), exclude)\n",
        "\n",
        "        for j in range(self.batchSize):\n",
        "            self.remainingMoves[j].remove(prediction)\n",
        "            indices = np.random.choice(choices, replace=False, size=int(self.mines))\n",
        "            mines.append(indices)\n",
        "            # Have to account for matrix being padded\n",
        "            scaledIndices = [i + 2*(i//self.width) for i in indices]\n",
        "            # Use the index of the mines to move the kernel to the correct spots\n",
        "            for index in scaledIndices:\n",
        "                self.solvedBoard[j][np.unravel_index([i + index for i in self.kernel], self.solvedBoard[0].shape)] += 1\n",
        "\n",
        "        # Unpad matrix\n",
        "        self.solvedBoard = np.delete(self.solvedBoard, [0,self.height+1], 1)\n",
        "        self.solvedBoard = np.delete(self.solvedBoard, [0,self.width+1], 2)\n",
        "        self.solvedBoard = np.reshape(self.solvedBoard, (self.batchSize, self.height*self.width))\n",
        "\n",
        "        # Set randomized mines\n",
        "        for j in range(self.batchSize):\n",
        "            for mine in mines[j]: self.remainingMoves[j].remove(mine)\n",
        "            self.solvedBoard[j][mines[j]] = 9\n",
        "            self.solutionBoard[j][mines[j]] = 0\n",
        "        self.solvedBoard = self.solvedBoard.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Network treats minesweeper as a multilabel multiclassification problem where\n",
        "# each grid can either be classified as a mine or space, output ideally maintains\n",
        "# the same shape as the input so we are limited to convolution layers only\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(100,(3,3), activation = 'relu', padding = 'same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(100,(3,3), activation = 'relu', padding = 'same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(100,(3,3), activation = 'relu', padding = 'same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(100,(3,3), activation = 'relu', padding = 'same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(100,(3,3), activation = 'relu', padding = 'same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(1,(3,3), activation = 'sigmoid', padding = 'same'),\n",
        "])\n",
        "optimizer = tf.keras.optimizers.RMSprop()"
      ],
      "metadata": {
        "id": "Dn3IU9xCIR8K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "if_KkJJjYSyC"
      },
      "outputs": [],
      "source": [
        "# Model accuracy during training performs better than testing as it is learning off\n",
        "# of the given solution sets, therefore we compute validation accuracy seperately\n",
        "def getAccuracy(model, height, width, mines, batchSize):\n",
        "    wins = np.repeat(True, batchSize)\n",
        "    sweep = MineSweeper(height, width, mines, batchSize)\n",
        "    y_pred = tf.reshape(model(sweep.board), (batchSize, height*width))\n",
        "    sweep.initializeBoard(tf.math.argmin(y_pred, axis=1).numpy()[0])\n",
        "    numMoves = height*width - mines - 1\n",
        "\n",
        "    for i in range(numMoves):\n",
        "        y_pred = tf.reshape(model(sweep.board), (batchSize, height*width))\n",
        "        choices = tf.math.argmax(y_pred - sweep.allowedChoices, axis=1).numpy()\n",
        "        for j, choice in enumerate(choices):\n",
        "            if sweep.solutionBoard[j][choice] == 0:\n",
        "                choices[j] = sweep.remainingMoves[j].getRandom()\n",
        "                wins[j] = False\n",
        "        for j, choice in enumerate(choices):\n",
        "            sweep.remainingMoves[j].remove(choice)\n",
        "            sweep.allowedChoices[j][choice] = 2\n",
        "            sweep.board[j][choice//width][choice%width][9] = 0\n",
        "            sweep.board[j][choice//width][choice%width][sweep.solvedBoard[j][choice]] = 1\n",
        "    return wins.sum()/(batchSize)\n",
        "\n",
        "# Customized gradient descent loop is required in order to update the Minesweeper boards during training\n",
        "# Model training is done by playing hundreds to thousands of games in tandem in accordance to the batch size,\n",
        "# every gradient descent loop it will predict a single move for each board and then run gradient descent\n",
        "# over that single move. This will be done until each game is complete then it will move onto the next batch of games.\n",
        "def trainModel(height, width, mines, games, batchSize, modelName):\n",
        "    numMoves = height*width - mines - 1\n",
        "    accuracy = []\n",
        "    wins = np.full((games, batchSize), True)\n",
        "\n",
        "    for game in range(games):\n",
        "        loss_cum = 0\n",
        "        sweep = MineSweeper(height, width, mines, batchSize)\n",
        "\n",
        "        # In classic minesweeper, the board is only initialized once you click on a space\n",
        "        # Therefore, the first prediction must be made before initializing the board\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = tf.reshape(model(sweep.board), (batchSize, height*width))\n",
        "            sweep.initializeBoard(tf.math.argmin(y_pred, axis=1).numpy()[0])\n",
        "            loss = tf.keras.losses.BinaryCrossentropy()(sweep.solutionBoard, y_pred)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Due to the nature of the model it will perform the same move more than once\n",
        "        # A simple preprocessing step using the allowedChoices board prevents this from happening\n",
        "        for i in range(numMoves):\n",
        "            with tf.GradientTape() as tape:\n",
        "                y_pred = tf.reshape(model(sweep.board), (batchSize, height*width))\n",
        "                choices = tf.math.argmax(y_pred - sweep.allowedChoices, axis=1).numpy()\n",
        "                for j, choice in enumerate(choices):\n",
        "                    if sweep.solutionBoard[j][choice] == 0:\n",
        "                        choices[j] = sweep.remainingMoves[j].getRandom()\n",
        "                        wins[game][j] = False\n",
        "                for j, choice in enumerate(choices):\n",
        "                    sweep.remainingMoves[j].remove(choice)\n",
        "                    sweep.allowedChoices[j][choice] = 2\n",
        "                    sweep.board[j][choice//width][choice%width][9] = 0\n",
        "                    sweep.board[j][choice//width][choice%width][sweep.solvedBoard[j][choice]] = 1\n",
        "                loss = tf.keras.losses.BinaryCrossentropy()(sweep.solutionBoard, y_pred)\n",
        "                loss_cum += loss\n",
        "            grads = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        valWinRate = round(100 * getAccuracy(model, height, width, mines, batchSize), 2)\n",
        "        winRate = round(100 * wins[game].sum()/batchSize, 2)\n",
        "        print(\"Batch \" + str(game + 1) + \" loss = \" + str(round(loss_cum.numpy(), 2))\n",
        "                + \"   Win rate: \" + str(winRate) + \"%\" + \"   Val Win rate: \" + str(valWinRate) + \"%\")\n",
        "        if game != 0 and valWinRate > np.max(accuracy):\n",
        "            model.save(modelName + '.keras')\n",
        "        accuracy.append(valWinRate)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YimcDzNjYSyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd614f1-4648-4323-ad91-3ff2039f44f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 loss = 23.35   Win rate: 0.0%   Val Win rate: 0.05%\n",
            "Batch 2 loss = 15.88   Win rate: 15.97%   Val Win rate: 16.5%\n",
            "Batch 3 loss = 14.39   Win rate: 39.26%   Val Win rate: 44.09%\n",
            "Batch 4 loss = 13.11   Win rate: 62.74%   Val Win rate: 63.38%\n",
            "Batch 5 loss = 12.56   Win rate: 73.19%   Val Win rate: 74.02%\n",
            "Batch 6 loss = 12.29   Win rate: 80.08%   Val Win rate: 77.73%\n",
            "Batch 7 loss = 12.06   Win rate: 82.71%   Val Win rate: 80.91%\n",
            "Batch 8 loss = 11.98   Win rate: 85.11%   Val Win rate: 82.32%\n",
            "Batch 9 loss = 11.86   Win rate: 86.08%   Val Win rate: 84.28%\n",
            "Batch 10 loss = 11.85   Win rate: 88.43%   Val Win rate: 86.08%\n",
            "Batch 11 loss = 11.78   Win rate: 87.01%   Val Win rate: 86.23%\n",
            "Batch 12 loss = 11.68   Win rate: 88.62%   Val Win rate: 83.35%\n",
            "Batch 13 loss = 12.02   Win rate: 87.26%   Val Win rate: 87.26%\n",
            "Batch 14 loss = 11.66   Win rate: 90.23%   Val Win rate: 88.18%\n",
            "Batch 15 loss = 11.6   Win rate: 91.55%   Val Win rate: 89.01%\n",
            "Batch 16 loss = 11.82   Win rate: 91.46%   Val Win rate: 87.21%\n",
            "Batch 17 loss = 11.73   Win rate: 89.89%   Val Win rate: 88.57%\n",
            "Batch 18 loss = 11.64   Win rate: 90.33%   Val Win rate: 87.84%\n",
            "Batch 19 loss = 11.57   Win rate: 91.36%   Val Win rate: 88.82%\n",
            "Batch 20 loss = 11.51   Win rate: 91.36%   Val Win rate: 91.02%\n",
            "Batch 21 loss = 11.49   Win rate: 93.51%   Val Win rate: 89.11%\n",
            "Batch 22 loss = 11.51   Win rate: 92.43%   Val Win rate: 89.4%\n",
            "Batch 23 loss = 11.5   Win rate: 92.82%   Val Win rate: 91.21%\n",
            "Batch 24 loss = 11.4   Win rate: 93.8%   Val Win rate: 90.38%\n",
            "Batch 25 loss = 11.49   Win rate: 93.02%   Val Win rate: 92.33%\n",
            "Batch 26 loss = 11.41   Win rate: 93.95%   Val Win rate: 91.6%\n",
            "Batch 27 loss = 11.42   Win rate: 94.04%   Val Win rate: 90.92%\n",
            "Batch 28 loss = 11.39   Win rate: 93.21%   Val Win rate: 91.55%\n",
            "Batch 29 loss = 11.52   Win rate: 93.31%   Val Win rate: 91.26%\n",
            "Batch 30 loss = 11.39   Win rate: 93.07%   Val Win rate: 92.92%\n",
            "Batch 31 loss = 11.51   Win rate: 94.43%   Val Win rate: 91.46%\n",
            "Batch 32 loss = 11.57   Win rate: 93.99%   Val Win rate: 91.6%\n",
            "Batch 33 loss = 11.51   Win rate: 95.61%   Val Win rate: 92.33%\n",
            "Batch 34 loss = 11.4   Win rate: 95.41%   Val Win rate: 92.87%\n",
            "Batch 35 loss = 11.46   Win rate: 94.63%   Val Win rate: 92.72%\n",
            "Batch 36 loss = 11.47   Win rate: 94.19%   Val Win rate: 90.53%\n",
            "Batch 37 loss = 11.32   Win rate: 93.95%   Val Win rate: 91.36%\n",
            "Batch 38 loss = 11.34   Win rate: 94.78%   Val Win rate: 92.33%\n",
            "Batch 39 loss = 11.32   Win rate: 94.58%   Val Win rate: 93.26%\n",
            "Batch 40 loss = 11.3   Win rate: 95.7%   Val Win rate: 92.82%\n",
            "Batch 41 loss = 11.29   Win rate: 94.82%   Val Win rate: 93.26%\n",
            "Batch 42 loss = 11.34   Win rate: 94.09%   Val Win rate: 93.26%\n",
            "Batch 43 loss = 11.34   Win rate: 95.12%   Val Win rate: 92.63%\n",
            "Batch 44 loss = 11.26   Win rate: 94.97%   Val Win rate: 92.29%\n",
            "Batch 45 loss = 11.27   Win rate: 94.63%   Val Win rate: 93.41%\n",
            "Batch 46 loss = 11.14   Win rate: 96.0%   Val Win rate: 92.09%\n",
            "Batch 47 loss = 11.35   Win rate: 95.51%   Val Win rate: 94.38%\n",
            "Batch 48 loss = 11.96   Win rate: 95.8%   Val Win rate: 92.33%\n",
            "Batch 49 loss = 11.32   Win rate: 93.9%   Val Win rate: 92.92%\n",
            "Batch 50 loss = 11.37   Win rate: 95.31%   Val Win rate: 92.04%\n",
            "Batch 51 loss = 11.22   Win rate: 93.85%   Val Win rate: 92.97%\n",
            "Batch 52 loss = 11.25   Win rate: 95.02%   Val Win rate: 93.46%\n",
            "Batch 53 loss = 11.32   Win rate: 95.36%   Val Win rate: 94.09%\n",
            "Batch 54 loss = 11.28   Win rate: 95.61%   Val Win rate: 93.75%\n",
            "Batch 55 loss = 11.17   Win rate: 95.36%   Val Win rate: 93.95%\n",
            "Batch 56 loss = 11.39   Win rate: 95.95%   Val Win rate: 93.99%\n",
            "Batch 57 loss = 11.32   Win rate: 95.21%   Val Win rate: 93.9%\n",
            "Batch 58 loss = 11.16   Win rate: 95.56%   Val Win rate: 93.99%\n",
            "Batch 59 loss = 11.39   Win rate: 96.44%   Val Win rate: 94.38%\n",
            "Batch 60 loss = 11.17   Win rate: 96.78%   Val Win rate: 93.26%\n",
            "Batch 61 loss = 11.09   Win rate: 96.73%   Val Win rate: 92.19%\n",
            "Batch 62 loss = 11.26   Win rate: 94.43%   Val Win rate: 92.87%\n",
            "Batch 63 loss = 11.23   Win rate: 94.92%   Val Win rate: 93.99%\n",
            "Batch 64 loss = 11.32   Win rate: 95.21%   Val Win rate: 92.82%\n",
            "Batch 65 loss = 11.24   Win rate: 96.29%   Val Win rate: 94.14%\n",
            "Batch 66 loss = 10.56   Win rate: 95.8%   Val Win rate: 93.55%\n",
            "Batch 67 loss = 11.42   Win rate: 95.9%   Val Win rate: 94.04%\n",
            "Batch 68 loss = 11.29   Win rate: 95.9%   Val Win rate: 94.82%\n",
            "Batch 69 loss = 11.36   Win rate: 95.7%   Val Win rate: 94.14%\n",
            "Batch 70 loss = 11.3   Win rate: 95.8%   Val Win rate: 94.24%\n",
            "Batch 71 loss = 11.28   Win rate: 95.75%   Val Win rate: 94.14%\n",
            "Batch 72 loss = 11.04   Win rate: 96.78%   Val Win rate: 94.14%\n",
            "Batch 73 loss = 11.14   Win rate: 95.31%   Val Win rate: 94.24%\n",
            "Batch 74 loss = 11.13   Win rate: 96.34%   Val Win rate: 93.26%\n",
            "Batch 75 loss = 11.22   Win rate: 94.97%   Val Win rate: 94.43%\n",
            "Batch 76 loss = 11.13   Win rate: 96.53%   Val Win rate: 94.63%\n",
            "Batch 77 loss = 11.36   Win rate: 95.95%   Val Win rate: 93.85%\n",
            "Batch 78 loss = 11.23   Win rate: 96.0%   Val Win rate: 93.07%\n",
            "Batch 79 loss = 11.15   Win rate: 94.82%   Val Win rate: 94.73%\n",
            "Batch 80 loss = 11.13   Win rate: 95.75%   Val Win rate: 94.53%\n",
            "Batch 81 loss = 11.46   Win rate: 96.09%   Val Win rate: 93.95%\n",
            "Batch 82 loss = 11.17   Win rate: 95.75%   Val Win rate: 94.87%\n",
            "Batch 83 loss = 11.16   Win rate: 96.44%   Val Win rate: 93.46%\n",
            "Batch 84 loss = 11.17   Win rate: 95.56%   Val Win rate: 95.21%\n",
            "Batch 85 loss = 11.28   Win rate: 96.44%   Val Win rate: 95.56%\n",
            "Batch 86 loss = 11.07   Win rate: 96.29%   Val Win rate: 95.36%\n",
            "Batch 87 loss = 11.14   Win rate: 97.02%   Val Win rate: 92.72%\n",
            "Batch 88 loss = 11.2   Win rate: 95.12%   Val Win rate: 93.8%\n",
            "Batch 89 loss = 11.15   Win rate: 95.21%   Val Win rate: 95.46%\n",
            "Batch 90 loss = 11.05   Win rate: 97.07%   Val Win rate: 95.07%\n",
            "Batch 91 loss = 11.26   Win rate: 95.31%   Val Win rate: 95.7%\n",
            "Batch 92 loss = 11.2   Win rate: 97.41%   Val Win rate: 94.53%\n",
            "Batch 93 loss = 11.49   Win rate: 96.63%   Val Win rate: 93.41%\n",
            "Batch 94 loss = 11.1   Win rate: 95.9%   Val Win rate: 95.85%\n",
            "Batch 95 loss = 11.24   Win rate: 96.48%   Val Win rate: 94.63%\n",
            "Batch 96 loss = 10.88   Win rate: 97.22%   Val Win rate: 93.7%\n",
            "Batch 97 loss = 11.17   Win rate: 96.88%   Val Win rate: 94.09%\n",
            "Batch 98 loss = 11.37   Win rate: 97.07%   Val Win rate: 93.8%\n",
            "Batch 99 loss = 11.15   Win rate: 95.61%   Val Win rate: 95.17%\n",
            "Batch 100 loss = 10.8   Win rate: 97.31%   Val Win rate: 94.63%\n"
          ]
        }
      ],
      "source": [
        "# Beginner, intermediate, and expert level boards preset and pretrained\n",
        "height, width, mines, games, batchSize = (9, 9, 10, 100, 2048)\n",
        "beginnerAccuracy = trainModel(height, width, mines, games, batchSize, modelName='BeginnerModel')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "height, width, mines, games, batchSize = (16, 16, 40, 20, 2048)\n",
        "intermediateAccuracy = trainModel(height, width, mines, games, batchSize, modelName='IntermediateModel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW0WrLF8xzJO",
        "outputId": "3dee230d-5aa1-4612-f01c-a34c2aa57fe1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 loss = 43.76   Win rate: 81.4%   Val Win rate: 77.25%\n",
            "Batch 2 loss = 43.57   Win rate: 84.18%   Val Win rate: 78.71%\n",
            "Batch 3 loss = 43.22   Win rate: 84.81%   Val Win rate: 78.12%\n",
            "Batch 4 loss = 43.49   Win rate: 85.69%   Val Win rate: 80.81%\n",
            "Batch 5 loss = 43.26   Win rate: 86.13%   Val Win rate: 80.57%\n",
            "Batch 6 loss = 42.97   Win rate: 86.04%   Val Win rate: 78.66%\n",
            "Batch 7 loss = 43.29   Win rate: 83.69%   Val Win rate: 77.98%\n",
            "Batch 8 loss = 43.0   Win rate: 87.7%   Val Win rate: 80.32%\n",
            "Batch 9 loss = 43.15   Win rate: 86.82%   Val Win rate: 77.54%\n",
            "Batch 10 loss = 43.14   Win rate: 84.42%   Val Win rate: 81.84%\n",
            "Batch 11 loss = 43.58   Win rate: 85.89%   Val Win rate: 80.22%\n",
            "Batch 12 loss = 43.18   Win rate: 84.38%   Val Win rate: 77.1%\n",
            "Batch 13 loss = 42.88   Win rate: 82.23%   Val Win rate: 81.64%\n",
            "Batch 14 loss = 42.97   Win rate: 86.38%   Val Win rate: 81.1%\n",
            "Batch 15 loss = 43.49   Win rate: 84.96%   Val Win rate: 81.35%\n",
            "Batch 16 loss = 43.18   Win rate: 87.6%   Val Win rate: 81.98%\n",
            "Batch 17 loss = 43.11   Win rate: 87.6%   Val Win rate: 80.81%\n",
            "Batch 18 loss = 43.3   Win rate: 86.38%   Val Win rate: 79.1%\n",
            "Batch 19 loss = 42.92   Win rate: 86.82%   Val Win rate: 82.32%\n",
            "Batch 20 loss = 42.87   Win rate: 88.13%   Val Win rate: 82.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "height, width, mines, games, batchSize = (16, 30, 99, 15, 2048)\n",
        "expertAccuracy = trainModel(height, width, mines, games, batchSize, modelName='ExpertModel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSoSbWeaypm5",
        "outputId": "7136cf2f-8b5e-479f-cb9f-91fd95d311cf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 loss = 94.59   Win rate: 41.94%   Val Win rate: 27.1%\n",
            "Batch 2 loss = 94.53   Win rate: 45.31%   Val Win rate: 30.71%\n",
            "Batch 3 loss = 94.54   Win rate: 45.36%   Val Win rate: 30.42%\n",
            "Batch 4 loss = 94.66   Win rate: 48.19%   Val Win rate: 29.79%\n",
            "Batch 5 loss = 94.26   Win rate: 42.87%   Val Win rate: 30.71%\n",
            "Batch 6 loss = 94.29   Win rate: 43.85%   Val Win rate: 30.96%\n",
            "Batch 7 loss = 94.22   Win rate: 42.48%   Val Win rate: 30.22%\n",
            "Batch 8 loss = 94.16   Win rate: 46.04%   Val Win rate: 30.71%\n",
            "Batch 9 loss = 94.3   Win rate: 43.12%   Val Win rate: 33.3%\n",
            "Batch 10 loss = 94.29   Win rate: 50.29%   Val Win rate: 32.23%\n",
            "Batch 11 loss = 94.13   Win rate: 46.78%   Val Win rate: 31.93%\n",
            "Batch 12 loss = 93.98   Win rate: 46.29%   Val Win rate: 31.25%\n",
            "Batch 13 loss = 94.55   Win rate: 46.14%   Val Win rate: 30.96%\n",
            "Batch 14 loss = 93.96   Win rate: 48.54%   Val Win rate: 33.01%\n",
            "Batch 15 loss = 94.02   Win rate: 47.07%   Val Win rate: 33.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beginnerModel = tf.keras.models.load_model('BeginnerModel.keras')\n",
        "print(\"Beginner board: \" + str(100 * getAccuracy(beginnerModel, 9, 9, 10, 2048)) + \"% win rate\")\n",
        "intermediateModel = tf.keras.models.load_model('IntermediateModel.keras')\n",
        "print(\"Intermediate board: \" + str(100 * getAccuracy(intermediateModel, 16, 16, 40, 2048)) + \"% win rate\")\n",
        "expertModel = tf.keras.models.load_model('ExpertModel.keras')\n",
        "print(\"Expert board: \" + str(100 * getAccuracy(expertModel, 16, 30, 99, 2048)) + \"% win rate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVXuY4h4yxYj",
        "outputId": "5ceff39f-b3af-457b-d0bc-1faee01ee192"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginner board: 95.01953125% win rate\n",
            "Intermediate board: 81.54296875% win rate\n",
            "Expert board: 33.203125% win rate\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2c7d21b8fc3c5bab5c49b39fec8e93f42c31e3b4d14dff554b686442b6fdc50f"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}